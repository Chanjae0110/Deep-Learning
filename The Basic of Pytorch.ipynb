{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 반드시 알아야 하는 파이토치 스킬\n",
    "#### 텐서\n",
    "Scalar: 우리가 흔히 알고 있는 상수 값. 하나의 값을 표현할 때 1개의 수치로 표현\n",
    "\n",
    "Vector: 하나의 값을 표현할 때 2개 이상의 수치로 표현\n",
    "\n",
    "Matrix: 2개 이상의 벡터 값을 통합해 구성된 값. 선형 대수의 기본 단위. 2차원의 배열\n",
    "\n",
    "Tensor: 2차원 이상의 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n",
      "tensor([[[ 9., 10.],\n",
      "         [11., 12.]],\n",
      "\n",
      "        [[13., 14.],\n",
      "         [15., 16.]]])\n"
     ]
    }
   ],
   "source": [
    "#텐서 값 간의 사칙연산 예제\n",
    "import torch\n",
    "tensor1 = torch.tensor([[[1.,2.], [3., 4.]], [[5., 6.], [7., 8,]]])\n",
    "print(tensor1)\n",
    "\n",
    "tensor2 = torch.tensor([[[9.,10.], [11., 12.]], [[13., 14.], [15., 16,]]])\n",
    "print(tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[10., 12.],\n",
      "         [14., 16.]],\n",
      "\n",
      "        [[18., 20.],\n",
      "         [22., 24.]]])\n"
     ]
    }
   ],
   "source": [
    "sum_tensor = tensor1 +tensor2\n",
    "print(sum_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-8., -8.],\n",
      "         [-8., -8.]],\n",
      "\n",
      "        [[-8., -8.],\n",
      "         [-8., -8.]]])\n",
      "tensor([[[  9.,  20.],\n",
      "         [ 33.,  48.]],\n",
      "\n",
      "        [[ 65.,  84.],\n",
      "         [105., 128.]]])\n",
      "tensor([[[0.1111, 0.2000],\n",
      "         [0.2727, 0.3333]],\n",
      "\n",
      "        [[0.3846, 0.4286],\n",
      "         [0.4667, 0.5000]]])\n"
     ]
    }
   ],
   "source": [
    "sub_tensor = tensor1 -tensor2\n",
    "print(sub_tensor)\n",
    "\n",
    "mul_tensor = tensor1 *tensor2\n",
    "print(mul_tensor)\n",
    "\n",
    "div_tensor = tensor1 /tensor2\n",
    "print(div_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch 모듈에 내장된 메서드로 사칙연산 + 내적연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10., 12.],\n",
       "         [14., 16.]],\n",
       "\n",
       "        [[18., 20.],\n",
       "         [22., 24.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-8., -8.],\n",
       "         [-8., -8.]],\n",
       "\n",
       "        [[-8., -8.],\n",
       "         [-8., -8.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sub(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  9.,  20.],\n",
       "         [ 33.,  48.]],\n",
       "\n",
       "        [[ 65.,  84.],\n",
       "         [105., 128.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1111, 0.2000],\n",
       "         [0.2727, 0.3333]],\n",
       "\n",
       "        [[0.3846, 0.4286],\n",
       "         [0.4667, 0.5000]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.div(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 31.,  34.],\n",
       "         [ 71.,  78.]],\n",
       "\n",
       "        [[155., 166.],\n",
       "         [211., 226.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#텐서 내적연산\n",
    "torch.matmul(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd & 관련 배경\n",
    "torch.autograd: 신경망 학습을 지원하는 PyTorch의 자동 미분엔진\n",
    "\n",
    "### 배경\n",
    "신경망(NN:Neural Network) 어떤 입력 데이터에 대해 실행되는 중첩(nested)된 함수들의 모음. 이 함수들은 PyTorch에서 Tensor로 저장되는, 가중치(weight)와 편향(bias)로 구성된 매개변수들로 정의됨\n",
    "\n",
    "#### 신경망 학습 방법\n",
    "순전파(Forward Propagation): 순전파 단계에서, 신경망은 정답을 맞추기 위해 최선의 추측(best guess)을 함. 추측을 위해 입력 데이터를 각 함수들에서 실행\n",
    "\n",
    "역전파(Backward Propagation): 역전파 단계에서, 신경망은 추측한 값에서 발생한 오류(error)에 비례하여(proportionate) 매개변수들을 적절히 조절함. 출력(output)로부터 역방향으로 이동하면서 오류에 대한 함수들의 매개변수들의 미분값(gradient)을 수집하고, 경사하강법(gradient descent)을 사용하여 매개변수들을 최적화 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 예제는 간단한 딥러닝 모델을 설계하고 방정식 내에 존재하는 파라미터를 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH_SIZE는 딥러닝 모델에서 파라미터를 업데이트할 때 계산되는 데이터 수\n",
    "\n",
    "INPUT_SIZE는 딥러닝 모델에서의 Input의 크기이자 입력층의 노드 수\n",
    "\n",
    "HIDDEN_SIZE는 딥러닝 모델에서 Input을 다수의 파라미터를 이용해 계산한 결과에 한 번 더 계산되는 파라미터 수\n",
    "\n",
    "OUTPUT_SIZE는 딥러닝 모델에서 최종으로 출력되는 값의 벡터의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "INPUT_SIZE = 1000\n",
    "HIDDEN_SIZE = 100\n",
    "OUTPUT_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randn 매서드를 이용해 데이터와 파라미터를 설정\n",
    "#randn은 평균이 0, 표준편차가 1인 정규분포에서 샘플링한 값으로, 데이터를 만든다는 것을 의미하며, 데이터를 만들어 낼 때 데이터 모양 설정 가능\n",
    "x = torch.randn(BATCH_SIZE, INPUT_SIZE, device = DEVICE, dtype = torch.float, requires_grad = False)\n",
    "y= torch.randn(BATCH_SIZE, OUTPUT_SIZE, device = DEVICE, dtype = torch.float, requires_grad = False)\n",
    "#업데이트할 파라미터 값을 설정\n",
    "w1 = torch.randn(INPUT_SIZE, HIDDEN_SIZE, device = DEVICE, dtype = torch.float, requires_grad = True)\n",
    "w2 = torch.randn(HIDDEN_SIZE, OUTPUT_SIZE, device = DEVICE, dtype = torch.float, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  100 \t Loss:  1.5029465430416167e-05\n",
      "Iteration:  200 \t Loss:  9.635405149310827e-06\n",
      "Iteration:  300 \t Loss:  6.993234819674399e-06\n",
      "Iteration:  400 \t Loss:  5.416171461547492e-06\n",
      "Iteration:  500 \t Loss:  4.43070075561991e-06\n"
     ]
    }
   ],
   "source": [
    "#learning_rate 설정에 따라 Gradient 값에 따른 학습 정도가 결정\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(1, 501):\n",
    "    y_pred = x.mm(w1).clamp(min = 0).mm(w2) \n",
    "    # Input x와 Parameter w1간의 행렬 곱을 이용해 나온 결과 값 계산\n",
    "    # clamp 매서드 활용해 비선형 함수를 적용\n",
    "    loss = (y_pred -y).pow(2).sum()\n",
    "    if t%100 == 0:\n",
    "        print(\"Iteration: \", t, \"\\t\", \"Loss: \", loss.item())\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500번의 반복문을 실행하면서 Loss 값이 줄어드는 것을 확인할 수 있음. Loss 값이 줄어든 다는 것은 Input이 w1과 w2를 통해 계산된 결과값과 y 값이 점점 비슷해진다는 것을 의미하며 y값과 비슷한 Output을 계산할 수 있도록 w1과 w2 계산된다는 것을 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
